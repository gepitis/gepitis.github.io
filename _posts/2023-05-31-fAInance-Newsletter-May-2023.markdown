---
layout: default
title:  "fAInance Newsletter - May 2023"
date:   2023-05-31 05:35:34 +0100 
categories: jekyll update
---

# fAInance Newsletter - May 2023

    
## ML for Finance/Quant:
 <details><summary><a href="https://venturebeat.com/ai/jpmorgan-plans-for-a-chatgpt-like-investment-service-are-just-part-of-its-larger-ai-ambitions">JPMorgan’s plans for a ChatGPT-like investment service are just part of its larger AI ambitions</a></summary>JPMorgan Chase is reportedly developing IndexGPT, a ChatGPT-like service to offer investment advice tailored to customer needs. The company has applied for a trademark, planning to utilize cloud computing software and artificial intelligence for analyzing and selecting securities. While IndexGPT is just one aspect of JPMorgan's AI ambitions, the company is making significant strides in AI adoption, with plans to deliver $1.5 billion in value through AI by the end of the year. JPMorgan boasts a substantial AI research team, and while they restrict certain AI tools, they recognize the importance of large language models like GPT in their AI efforts.</details>
 <details><summary><a href="https://arxiv.org/abs/2305.01505">Beyond Classification: Financial Reasoning in State-of-the-Art Language Models</a></summary>This research explores the application of Large Language Models (LLMs) with 100 billion or more parameters in the financial domain. The study demonstrates that LLMs with 6B parameters or more, combined with instruction-tuning and larger datasets, exhibit the ability to generate coherent financial reasoning, contributing to the understanding of the efficacy of language models in the field of finance and investment decision-making.</details>
 <details><summary><a href="https://www.sciencedirect.com/science/article/abs/pii/S0022199623000594">Credit growth, the yield curve and financial crisis prediction: Evidence from a machine learning approach☆</a></summary>The paper develops early warning models for financial crisis prediction using machine learning on macrofinancial data for 17 countries from 1870 to 2016. Machine learning models outperform traditional regression in forecasting crises, and credit growth and the slope of the yield curve (both domestically and globally) are identified as the most crucial predictors. The study's unique approach using the Shapley value framework provides economic insights and interprets the complex relationships between predictors and crisis risk.</details>
 <details><summary><a href="https://www.cell.com/heliyon/pdf/S2405-8440(23)03362-5.pdf">Optimization of investment strategies through machine learning</a></summary>This research endeavors to devise an advanced and sustainable stock quantitative investing model, employing a fusion of Machine Learning techniques and Economic Value-Added methodologies to optimize investment strategies, with a focus on quantitative stock selection through principal component analysis and economic value-added criteria, and algorithmic trading utilizing Moving Average Convergence, Stochastic Indicators, and Long-Short Term Memory, achieving superior forecasting accuracy with LSTM networks and outperforming the market by generating considerable returns, thus proving its viability for rational and profitable investing in various market situations. </details>
 <details><summary><a href="https://onlinelibrary.wiley.com/doi/full/10.1111/mafi.12382">Recent advances in reinforcement learning in finance</a></summary>The finance industry has undergone significant changes due to the abundance of data, leading to advancements in data processing and analysis. Reinforcement learning (RL) has emerged as a powerful tool in making financial decisions by leveraging large datasets with fewer model assumptions. Unlike traditional stochastic control theory, RL methods can handle complex financial environments and improve decision-making without heavily relying on specific model assumptions. This survey paper provides an overview of recent developments and applications of RL in finance, covering various algorithms, including value- and policy-based methods, and their connection with neural networks. The paper explores how RL techniques are applied in decision-making problems in finance, such as optimal execution, portfolio optimization, option pricing and hedging, market making, smart order routing, and robo-advising. The survey concludes by suggesting potential future research directions in this rapidly evolving field.</details>
 <details><summary><a href="https://arxiv.org/pdf/2305.07466.pdf">Systematic Review on Reinforcement Learning in the field of Fintech</a></summary>Reinforcement Learning (RL) has gained admiration in the field of Financial Technology (Fintech) due to its competence in addressing complex challenges. This systematic survey explores the correlation between RL and Fintech, focusing on prediction accuracy, complexity, scalability, risks, profitability, and performance. RL has shown remarkable results in various applications in finance, such as portfolio optimization, credit risk reduction, investment capital management, and profit maximization. The survey analyzes the contribution of RL-based strategies to financial institutions' performance, highlighting their superiority over other algorithms. The study discusses the use of RL algorithms in diverse decision-making challenges in Fintech and emphasizes their potential benefits in areas like Robo-advising, smart order channelling, market making, and portfolio optimization.</details>
 <details><summary><a href="https://arxiv.org/abs/2305.03835">Spatiotemporal Transformer for Stock Movement Prediction</a></summary>The finance industry has undergone significant changes due to the abundance of data, leading to advancements in data processing and analysis. Reinforcement learning (RL) has emerged as a powerful tool in making financial decisions by leveraging large datasets with fewer model assumptions. Unlike traditional stochastic control theory, RL methods can handle complex financial environments and improve decision-making without heavily relying on specific model assumptions. This survey paper provides an overview of recent developments and applications of RL in finance, covering various algorithms, including value- and policy-based methods, and their connection with neural networks. The paper explores how RL techniques are applied in decision-making problems in finance, such as optimal execution, portfolio optimization, option pricing and hedging, market making, smart order routing, and robo-advising. The survey concludes by suggesting potential future research directions in this rapidly evolving field.</details>
 <details><summary><a href="https://arxiv.org/pdf/2303.13216.pdf">A Case Study on AI Engineering Practices: Developing an Autonomous Stock Trading System</a></summary>This paper presents a case study on the development of an autonomous stock trading system using machine learning. It applies 10 AI engineering practices, documenting their effectiveness and challenges faced during development. The study aims to provide practical insights for AI engineering teams and newcomers in the field, contributing to the emerging body of evidence in AI engineering.</details>
 <details><summary><a href="https://arxiv.org/pdf/2305.16364.pdf">E2EAI: End-to-End Deep Learning Framework for Active Investing</a></summary>This paper introduces E2EAI, an end-to-end deep learning framework for active investing in the financial market. The framework covers factor selection, stock selection, and portfolio construction, optimizing portfolio returns subject to constraints. It utilizes a gated-attention mechanism for factor selection and portfolio construction, and a deep multifactor model with a directional estimator to interpret the deep factor. The framework is validated with real data, outperforming existing investment pipelines with separately optimized modules. The proposed E2E approach improves portfolio construction and provides insights into the relationship between original factors and the deep factor used in portfolio construction.</details>
 <details><summary><a href="https://arxiv.org/pdf/2305.14368.pdf">Support for Stock Trend Prediction Using Transformers and Sentiment Analysis</a></summary>This paper presents a Transformer-based model for accurate stock trend prediction using technical stock data and sentiment analysis. The model outperforms conventional Recurrent Neural Networks (RNNs) in capturing long-term dependencies and accounting for the impact of breaking news on stock movements. The experiments show significant improvements in directional accuracy, up to 18.63%, when using longer time sequences. The proposed model's performance is compared to RNNs for various trading strategies, demonstrating its effectiveness in predicting stock trends.</details>
 <a href="https://gmarti.gitlab.io//qfin/2023/05/28/qpm-stat-arb.html">[Active Reading with ChatGPT] Quantitative Portfolio Management: The Art and Science of Statistical Arbitrage</a>
 <a href="https://gmarti.gitlab.io//quant/2023/05/07/wikipedia-network-companies-sentence-transformers.html">Building a S&P 500 company classification from Wikipedia articles (guided by ChatGPT)</a>

    
## General ML:
 <details><summary><a href="https://arxiv.org/abs/2206.14486v1">Beyond neural scaling laws: beating power law scaling via data pruning</a></summary>The paper discusses neural scaling laws in deep learning, where error reduction follows a power law with increased training data or model size. However, this approach requires significant compute and energy costs. The paper proposes an alternative strategy of data pruning, where datasets are reduced while maintaining performance. The authors present a theoretical framework and empirical results showing that exponential scaling is achievable with data pruning, and they introduce a new self-supervised pruning metric that performs well without requiring labeled data. This work suggests that intelligent data pruning could lead to more resource-efficient deep learning.</details>
 <details><summary><a href="https://arxiv.org/abs/2304.12526">Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models</a></summary>The paper introduces Patch Diffusion, a new training framework for diffusion models that significantly reduces training time and improves data efficiency. By learning a conditional score function at the patch level, incorporating patch location information, and diversifying patch sizes throughout training, Patch Diffusion achieves more than 2× faster training while maintaining comparable or better generation quality. The approach democratizes diffusion model training, making it more accessible to researchers who lack high-end computational resources. Patch Diffusion also shows competitive results and notable performance gains even with small training datasets.</details>
 <details><summary><a href="https://developer.nvidia.com/blog/why-automatic-augmentation-matters/">Why Automatic Augmentation Matters</a></summary>Automatic data augmentation methods, such as AutoAugment and RandAugment, have emerged to reduce the reliance on manual data preprocessing for deep learning models. NVIDIA DALI, a powerful library, provides GPU-accelerated capabilities for data preprocessing, overcoming bottlenecks and improving training throughput. The library offers ready-to-use implementations of popular automatic augmentations, allowing for efficient training with diverse and augmented data. By moving data loading and augmentation to the GPU, DALI overcomes preprocessing bottlenecks, increasing GPU utilization and speeding up training times. Overall, automatic data augmentation with NVIDIA DALI provides an effective and efficient approach to diversify datasets and improve model accuracy in deep learning applications.</details>
 <details><summary><a href="https://arxiv.org/pdf/2305.01625.pdf">Unlimiformer: Long-Range Transformers with Unlimited Length Input</a></summary>The paper introduces Unlimiformer, a novel approach that extends the input length of existing pretrained encoder-decoder transformers without the need for retraining or modifying their architecture. Traditional transformers have been limited in input length due to their quadratic complexity in attending to every token in the input. Unlimiformer offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, enabling the model to attend only to the top-k keys in the input sequence. This allows Unlimiformer to process practically unlimited input sequences while maintaining sub-linear time complexity for querying the kNN index. By applying Unlimiformer to pretrained models such as BART and Longformer, the authors demonstrate that the models can process extremely long inputs without additional learned weights and without retraining. Unlimiformer is evaluated on various long-document and book-summarization benchmarks, showcasing its ability to process inputs of up to 500k tokens from the BookSum dataset without input truncation at test time. The proposed approach improves existing pretrained models and outperforms strong long-range transformers such as LED, PRIMERA, SLED, and Memorizing Transformers on long-range datasets. It also provides a generic solution that can be easily applied to trained models to extend their input length without requiring additional training or architectural changes. The code and models for Unlimiformer are made publicly available for further research.</details>
 <details><summary><a href="https://arxiv.org/pdf/2305.13048.pdf">RWKV: Reinventing RNNs for the Transformer Era</a></summary>The paper presents RWKV, a novel model architecture that aims to strike a balance between the computational efficiency of Transformers and the scalability of RNNs in sequence processing tasks. While Transformers have demonstrated their effectiveness in handling long-range dependencies, they suffer from quadratic scaling in terms of memory and computation with sequence length. On the other hand, RNNs exhibit linear scaling but struggle to match the performance of Transformers due to issues like the vanishing gradient problem. RWKV introduces a linear attention mechanism that allows it to be formulated as either a Transformer or an RNN. During training, the model benefits from the parallelizable computations of Transformers, while during inference, it maintains constant computational and memory complexity similar to RNNs. This efficient design enables RWKV to be scaled to tens of billions of parameters, making it the first non-Transformer architecture to achieve such scalability. The proposed linear attention in RWKV replaces the traditional dot-product attention found in Transformers with channel-directed attention. This change enhances efficiency without relying on approximation techniques. The authors show that RWKV performs on par with similarly sized Transformers in various experiments, highlighting its potential for creating more efficient models in the future. Overall, RWKV is a significant step towards reconciling the trade-offs between computational efficiency and model performance in sequence processing tasks, offering a promising solution for handling large-scale models and long input sequences.</details>


## Language Models:
 <details><summary><a href="https://www.spectator.co.uk/article/we-may-be-history-geoffrey-hinton-on-the-dangers-of-ai/?utm_source=ONTRAPORT-email-broadcast&utm_medium=ONTRAPORT-email-broadcast&utm_term=Newsletter&utm_content=Data+Science+Insider%3A+May+5th%2C+2023&utm_campaign=06052023">The godfather of AI: why I left Google</a></summary>Geoffrey Hinton, a prominent figure in AI, has expressed concerns that AI could pose an existential threat to humanity and may replace us as the dominant form of intelligence. He points out the potential dangers of digital intelligence developing sub-goals that do not align with human objectives, making us vulnerable to AI manipulation. Hinton believes that AI's ability to create more copies of itself could lead to a competitive evolution among AGI, potentially leading to the extinction of less powerful forms of intelligence.</details>
<details><summary><a href="https://arxiv.org/pdf/2305.10601.pdf">Tree of Thoughts: Deliberate Problem Solving with Large Language Models</a></summary>Language models like GPT have shown impressive capabilities across various tasks but are limited to token-level, left-to-right decision-making during inference. To address this, researchers introduce "Tree of Thoughts" (ToT), a novel framework for language model inference. ToT allows language models to make deliberate decisions by considering multiple reasoning paths and self-evaluating choices, enabling exploration and strategic lookahead during problem-solving. By maintaining a tree of coherent language sequences as intermediate steps, ToT significantly enhances problem-solving abilities on tasks like Game of 24, Creative Writing, and Mini Crosswords. ToT employs search heuristics through language-based self-evaluation and deliberation, outperforming existing methods. Empirical results demonstrate ToT's flexibility and generalizability in supporting different thought levels, generation and evaluation methods, and search algorithms, enhancing language models' problem-solving capabilities.</details>
<details><summary><a href="https://huggingface.co/blog/starcoder">StarCoder: A State-of-the-Art LLM for Code</a></summary>StarCoder and StarCoderBase are Large Language Models (LLMs) for code, trained on GitHub data and programming languages. They outperform existing open Code LLMs on various benchmarks and can process more input than any other open LLM, making them suitable for diverse applications, such as acting as a technical assistant, code autocompletion, code modification, and code explanation. StarCoder has undergone evaluations, surpassing other models in performance, and it is released under the OpenRAIL license, ensuring safety and ease of integration into products.</details>
<details><summary><a href="https://www.theverge.com/2023/5/23/23732454/microsoft-ai-windows-11-copilot-build">Microsoft announces Windows Copilot, an AI ‘personal assistant’ for Windows 11</a></summary>Microsoft is introducing a new AI assistant called Windows Copilot to Windows 11. Similar to Copilot sidebars in other Microsoft apps, this AI assistant will be integrated directly into Windows 11 and accessible from the taskbar across all applications. Windows Copilot acts as a personal assistant, offering assistance, summarizing content, rewriting or explaining it, and answering general questions. It will have its own dedicated space on the taskbar, separate from the search bar, and will allow users to perform various actions on their PC, such as adjusting settings. Since it's built on the same foundation as Bing Chat, developers can extend their plug-ins to this AI-powered assistant, providing a wide range of new functionalities. Windows Copilot will be publicly tested in June and later rolled out more broadly to existing Windows 11 users.</details>
<details><summary><a href="https://arxiv.org/pdf/2304.15004.pdf">Are Emergent Abilities of Large Language Models a Mirage?</a></summary>The paper challenges the notion of "emergent abilities" in large language models (LLMs), which are abilities that seem to appear abruptly and unpredictably as the model scales up. The authors propose an alternative explanation for these emergent abilities, suggesting that they may be a result of the researcher's choice of metric rather than fundamental changes in model behavior with scale. They demonstrate this alternative explanation through a simple mathematical model and then conduct three complementary analyses: They test their alternative hypotheses on the InstructGPT/GPT-3 model family, making and confirming predictions about the effect of metric choice on tasks with claimed emergent abilities. They meta-analyze published benchmarks of emergent abilities on BIG-Bench, revealing that the emergence phenomenon is closely tied to specific metrics rather than the model family on particular tasks. They intentionally change metrics for evaluation in various vision tasks, leading to seemingly emergent abilities in multiple architectures. Based on these analyses, the authors provide evidence that alleged emergent abilities in LLMs may be artifacts of the chosen metrics rather than a fundamental property of scaling AI models. They suggest that changing the metrics used for evaluation can reveal smooth, continuous, and predictable improvements in model performance, challenging the notion of emergent abilities in large language models.</details>
<details><summary><a href="https://arxiv.org/pdf/2305.14314.pdf">QLORA: Efficient Finetuning of Quantized LLMs</a></summary>The paper presents QLORA, an efficient fine-tuning approach for large language models that significantly reduces memory usage while preserving performance. QLORA uses 4-bit quantization to train a 65B parameter model on a single 48GB GPU without sacrificing performance. The method introduces innovations like 4-bit NormalFloat, Double Quantization, and Paged Optimizers to manage memory spikes during training. The resulting model family, called Guanaco, outperforms previously released models on the Vicuna benchmark and achieves state-of-the-art results with significantly reduced memory requirements. The paper also includes an in-depth study of instruction finetuning and chatbot performance using QLORA, showing the importance of data quality and dataset suitability for tasks. The study analyzes chatbot performance using both human and model-based evaluations and provides a qualitative analysis of Guanaco models' successes and failures. All model generations and code are publicly released for further research.</details>
<details><summary><a href="https://lightning.ai/blog/how-to-finetune-gpt-like-large-language-models-on-a-custom-dataset/">How To Finetune GPT Like Large Language Models on a Custom Dataset</a></summary>The article provides step-by-step instructions on how to fine-tune your own large language model using Lightning AI's Lit-GPT implementation. The process involves installing Lit-GPT, downloading the model weights, and preparing the dataset for fine-tuning. The tutorial uses the Dolly 2.0 instruction dataset for fine-tuning, which is processed in the Lit-GPT format and then used in the fine-tuning script. To get started, you need to clone the Lit-GPT repository and install its dependencies, including FlashAttention from PyTorch Nightly 2.1. After installing, you can download the RedPajama-INCITE 3B parameter weights and convert them to the Lit-Parrot format. The next step is to prepare the dataset by renaming keys in the JSON Lines format and tokenizing the data. This is achieved using a modified version of the Alpaca script provided in the tutorial. Finally, you can start the fine-tuning process by running the finetune/adapter.py script with the specified data path. You can also play with your fine-tuned model using the generate/adapter.py script to try different prompts and settings.The article also provides additional resources to learn more about large language models and efficient fine-tuning techniques, including guides on Falcon, mixed-precision techniques, and Parameter-Efficient LLM Finetuning with Low-Rank Adaptation (LoRA). The Lightning AI community's Discord is available for further discussions and questions about using Lit-Parrot.</details>
<details><summary><a href="https://github.com/iryna-kondr/scikit-llm">Scikit-LLM: Sklearn Meets Large Language Models</a></summary>Seamlessly integrate powerful language models like ChatGPT into scikit-learn for enhanced text analysis tasks.</details>
<details><summary><a href="https://arxiv.org/pdf/2305.12544.pdf">A PhD Student’s Perspective on Research in NLP in the Era of Very Large Language Models</a></summary>Recent progress in large language models has enabled the deployment of many generative NLP applications. At the same time, it has also led to a misleading public discourse that “it’s all been solved.” Not surprisingly, this has in turn made many NLP researchers – especially those at the beginning of their career – won-der about what NLP research area they should focus on. This document is a compilation ofNLP research directions that are rich for exploration, reflecting the views of a diverse group of PhD students in an academic research lab. While we identify many research areas, many others exist; we do not cover those areas that are currently addressed by LLMs but where LLMs lag behind in performance, or those focused on LLM development. We welcome suggestions for other research directions to include: https://bit.ly/nlp-era-llm</details>

    
## Miscellaneous:

<a href="https://www.modular.com/mojo">Mojo programming language for AI</a>

<a href="https://github.com/gventuri/pandas-ai">Pandas AI toolbox</a>

<a href="https://github.com/togethercomputer/OpenChatKit">OpenChatKit</a>

    
## Upcoming Conferences/Deadlines:
   