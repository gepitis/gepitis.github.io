---
layout: default
title:  "fAInance Newsletter - May 2023"
date:   2023-05-31 05:35:34 +0100 
categories: jekyll update
---

# fAInance Newsletter - May 2023
 
## ML for Finance/Quant:
 <details><summary><a href="https://arxiv.org/abs/2305.01505">Beyond Classification: Financial Reasoning in State-of-the-Art Language Models</a></summary>This research explores the application of Large Language Models (LLMs) with 100 billion or more parameters in the financial domain. The study demonstrates that LLMs with 6B parameters or more, combined with instruction-tuning and larger datasets, exhibit the ability to generate coherent financial reasoning, contributing to the understanding of the efficacy of language models in the field of finance and investment decision-making.</details>
 <details><summary><a href="https://www.sciencedirect.com/science/article/abs/pii/S0022199623000594">Credit growth, the yield curve and financial crisis prediction: Evidence from a machine learning approach☆</a></summary>The paper develops early warning models for financial crisis prediction using machine learning on macrofinancial data for 17 countries from 1870 to 2016. Machine learning models outperform traditional regression in forecasting crises, and credit growth and the slope of the yield curve (both domestically and globally) are identified as the most crucial predictors. The study's unique approach using the Shapley value framework provides economic insights and interprets the complex relationships between predictors and crisis risk.</details>
 <details><summary><a href="https://www.cell.com/heliyon/pdf/S2405-8440(23)03362-5.pdf">Optimization of investment strategies through machine learning</a></summary>This research endeavors to devise an advanced and sustainable stock quantitative investing model, employing a fusion of Machine Learning techniques and Economic Value-Added methodologies to optimize investment strategies, with a focus on quantitative stock selection through principal component analysis and economic value-added criteria, and algorithmic trading utilizing Moving Average Convergence, Stochastic Indicators, and Long-Short Term Memory, achieving superior forecasting accuracy with LSTM networks and outperforming the market by generating considerable returns, thus proving its viability for rational and profitable investing in various market situations. </details>
 <a href="https://gmarti.gitlab.io//qfin/2023/05/28/qpm-stat-arb.html">[Active Reading with ChatGPT] Quantitative Portfolio Management: The Art and Science of Statistical Arbitrage</a>
 <a href="https://gmarti.gitlab.io//quant/2023/05/07/wikipedia-network-companies-sentence-transformers.html">Building a S&P 500 company classification from Wikipedia articles (guided by ChatGPT)</a>



## Important ML R&D:
 <details><summary><a href="https://arxiv.org/abs/2206.14486v1">Beyond neural scaling laws: beating power law scaling via data pruning</a></summary>The paper discusses neural scaling laws in deep learning, where error reduction follows a power law with increased training data or model size. However, this approach requires significant compute and energy costs. The paper proposes an alternative strategy of data pruning, where datasets are reduced while maintaining performance. The authors present a theoretical framework and empirical results showing that exponential scaling is achievable with data pruning, and they introduce a new self-supervised pruning metric that performs well without requiring labeled data. This work suggests that intelligent data pruning could lead to more resource-efficient deep learning.</details>
 <details><summary><a href="https://arxiv.org/abs/2304.12526">Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models</a></summary>The paper introduces Patch Diffusion, a new training framework for diffusion models that significantly reduces training time and improves data efficiency. By learning a conditional score function at the patch level, incorporating patch location information, and diversifying patch sizes throughout training, Patch Diffusion achieves more than 2× faster training while maintaining comparable or better generation quality. The approach democratizes diffusion model training, making it more accessible to researchers who lack high-end computational resources. Patch Diffusion also shows competitive results and notable performance gains even with small training datasets.</details>
 <details><summary><a href="https://developer.nvidia.com/blog/why-automatic-augmentation-matters/">Why Automatic Augmentation Matters</a></summary>Automatic data augmentation methods, such as AutoAugment and RandAugment, have emerged to reduce the reliance on manual data preprocessing for deep learning models. NVIDIA DALI, a powerful library, provides GPU-accelerated capabilities for data preprocessing, overcoming bottlenecks and improving training throughput. The library offers ready-to-use implementations of popular automatic augmentations, allowing for efficient training with diverse and augmented data. By moving data loading and augmentation to the GPU, DALI overcomes preprocessing bottlenecks, increasing GPU utilization and speeding up training times. Overall, automatic data augmentation with NVIDIA DALI provides an effective and efficient approach to diversify datasets and improve model accuracy in deep learning applications.
</details>



## Important Language Model R&D:
 <details><summary><a href="https://www.spectator.co.uk/article/we-may-be-history-geoffrey-hinton-on-the-dangers-of-ai/?utm_source=ONTRAPORT-email-broadcast&utm_medium=ONTRAPORT-email-broadcast&utm_term=Newsletter&utm_content=Data+Science+Insider%3A+May+5th%2C+2023&utm_campaign=06052023
">The godfather of AI: why I left Google</a></summary>Geoffrey Hinton, a prominent figure in AI, has expressed concerns that AI could pose an existential threat to humanity and may replace us as the dominant form of intelligence. He points out the potential dangers of digital intelligence developing sub-goals that do not align with human objectives, making us vulnerable to AI manipulation. Hinton believes that AI's ability to create more copies of itself could lead to a competitive evolution among AGI, potentially leading to the extinction of less powerful forms of intelligence.</details>
<details><summary><a href="https://huggingface.co/blog/starcoder">StarCoder: A State-of-the-Art LLM for Code</a></summary>StarCoder and StarCoderBase are Large Language Models (LLMs) for code, trained on GitHub data and programming languages. They outperform existing open Code LLMs on various benchmarks and can process more input than any other open LLM, making them suitable for diverse applications, such as acting as a technical assistant, code autocompletion, code modification, and code explanation. StarCoder has undergone evaluations, surpassing other models in performance, and it is released under the OpenRAIL license, ensuring safety and ease of integration into products.</details>

## Miscellaneous:
- 

## Upcoming Conferences:
- 
